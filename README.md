# Vision Transformers

This repository contains my personal learning project on Vision Transformers (ViTs), a state-of-the-art architecture in computer vision that adapts Transformer models originally developed for NLP to image data.
The project includes a Jupyter notebook where I follow research papers,video tutorials and websites that explore the ViT architecture, and experiment with image classification tasks. ViTs leverage multi-headed self-attention mechanisms to capture complex visual patterns by processing images as sequences of patches, enabling both local and global feature learning.

This work builds on my background in convolutional neural networks and NLP transformers, aiming to deepen my understanding of how these concepts integrate for vision tasks.

---
The Jupyter Notebook can be found in the same repo:
<p align="center">
  <a href="Vision_Transformer_Learning.ipynb">
    <img src="https://img.shields.io/badge/Read-More-blue?style=for-the-badge&logo=readthedocs"/>
  </a>
</p>

---

### Tools & Libraries

<table>
  <tr>
    <td><a href="https://pytorch.org/" target="_blank"><img src="https://pytorch.org/assets/images/pytorch-logo.png" width="60" alt="PyTorch"/></a></td>
    <td><a href="https://matplotlib.org/" target="_blank"><img src="https://matplotlib.org/_static/images/logo2.svg" width="60" alt="Matplotlib"/></a></td>
    <td><a href="https://www.python.org/" target="_blank"><img src="https://www.python.org/static/community_logos/python-logo.png" width="60" alt="Python"/></a></td>
  </tr>
</table>

### What I Learned

- How Transformer architectures are adapted from NLP to vision by treating images as sequences of patches.
- The role of multi-headed self-attention in capturing both local and global image features.
- Differences and advantages of Vision Transformers over traditional CNNs in certain tasks.
- Practical experience implementing ViT-based image classification.
